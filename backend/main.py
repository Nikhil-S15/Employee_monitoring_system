from fastapi import FastAPI, WebSocket, HTTPException, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from sqlalchemy import create_engine, Column, Integer, String, DateTime, Float
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
from datetime import datetime, timedelta
from typing import List, Optional
import base64
import json
import asyncio
from pydantic import BaseModel
import os
from dotenv import load_dotenv
import random

# Load environment variables
load_dotenv()

# Database Setup
SQLALCHEMY_DATABASE_URL = os.getenv("DATABASE_URL", "sqlite:///./employee_monitoring.db")
engine = create_engine(SQLALCHEMY_DATABASE_URL, connect_args={"check_same_thread": False})
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base = declarative_base()

# Database Models
class DetectionLog(Base):
    __tablename__ = "detection_logs"
    
    id = Column(Integer, primary_key=True, index=True)
    employee_id = Column(String, default="EMP001")
    timestamp = Column(DateTime, default=datetime.utcnow)
    is_present = Column(Integer)  # 1 for present, 0 for not present
    emotion = Column(String, nullable=True)
    confidence = Column(Float, nullable=True)

Base.metadata.create_all(bind=engine)

# Pydantic Models
class DetectionResponse(BaseModel):
    id: int
    employee_id: str
    timestamp: datetime
    is_present: bool
    emotion: Optional[str]
    confidence: Optional[float]

class AnalyticsResponse(BaseModel):
    total_detections: int
    presence_percentage: float
    emotion_distribution: dict
    working_hours: float

# FastAPI App
app = FastAPI(
    title="Employee Monitoring System",
    description="Real-time employee presence and emotion detection system",
    version="1.0.0"
)

# CORS Middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Import computer vision libraries with error handling
try:
    import cv2
    import numpy as np
    CV_AVAILABLE = True
    print("‚úÖ OpenCV and NumPy imported successfully")
except ImportError as e:
    print(f"‚ùå Computer vision libraries not available: {e}")
    CV_AVAILABLE = False

# Import DeepFace with error handling
try:
    from deepface import DeepFace
    DEEPFACE_AVAILABLE = True
    print("‚úÖ DeepFace imported successfully")
except ImportError as e:
    print(f"‚ùå DeepFace not available: {e}")
    DEEPFACE_AVAILABLE = False
except Exception as e:
    print(f"‚ùå DeepFace compatibility issue: {e}")
    DEEPFACE_AVAILABLE = False

# Import FER with error handling
try:
    from fer import FER
    FER_AVAILABLE = True
    print("‚úÖ FER imported successfully")
except ImportError as e:
    print(f"‚ùå FER not available: {e}")
    FER_AVAILABLE = False

# Add this import at the top of main.py
try:
    from face_analyzer import face_analyzer
    FACE_ANALYZER_AVAILABLE = True
    print("‚úÖ Face analyzer imported successfully")
except ImportError as e:
    print(f"‚ùå Face analyzer not available: {e}")
    FACE_ANALYZER_AVAILABLE = False

# Enhanced Emotion Detector Class
class EnhancedEmotionDetector:
    def __init__(self):
        self.camera = None
        self.face_cascade = None
        self.is_monitoring = False
        self.emotions = ['happy', 'sad', 'neutral', 'angry', 'surprise', 'fear', 'disgust']
        
        if CV_AVAILABLE:
            try:
                self.face_cascade = cv2.CascadeClassifier(
                    cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
                )
                print("‚úÖ Face cascade classifier loaded")
            except Exception as e:
                print(f"‚ùå Failed to load face cascade: {e}")
        else:
            print("‚ùå OpenCV not available - face detection disabled")
        
    def initialize_camera(self):
        if not CV_AVAILABLE:
            print("‚ùå Camera initialization failed: OpenCV not available")
            return False
            
        if self.camera is None:
            try:
                self.camera = cv2.VideoCapture(0)
                if not self.camera.isOpened():
                    # Try different camera indices
                    for i in range(1, 4):
                        self.camera = cv2.VideoCapture(i)
                        if self.camera.isOpened():
                            break
                
                if self.camera.isOpened():
                    self.camera.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
                    self.camera.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
                    print("‚úÖ Camera initialized successfully")
                    return True
                else:
                    print("‚ùå Could not initialize camera")
                    return False
            except Exception as e:
                print(f"‚ùå Camera initialization error: {e}")
                return False
        return self.camera.isOpened()
    
    def detect_emotion_enhanced(self, face_roi):
        """Enhanced emotion detection with face analyzer"""
        if FACE_ANALYZER_AVAILABLE:
            try:
                return face_analyzer.detect_emotion(face_roi)
            except Exception as e:
                print(f"Face analyzer error: {e}")
        
        # Fallback to basic enhanced detection
        return self._detect_emotion_basic(face_roi)

    def _detect_emotion_basic(self, face_roi):
        """Basic enhanced emotion detection with persistence"""
        try:
            gray = cv2.cvtColor(face_roi, cv2.COLOR_BGR2GRAY)
            brightness = np.mean(gray)
            contrast = np.std(gray)
            
            # Initialize persistent emotion
            if not hasattr(self, 'persistent_emotion'):
                self.persistent_emotion = 'neutral'
                self.persistent_confidence = 85.0
                self.last_change_time = datetime.now()
            
            # Only consider changing emotion every 2+ seconds
            current_time = datetime.now()
            time_since_change = (current_time - self.last_change_time).total_seconds()
            
            if time_since_change < 2:  # Minimum emotion duration
                return self.persistent_emotion, self.persistent_confidence
            
            # Determine target emotion
            if brightness > 160 and contrast > 55:
                target_emotion = 'happy'
                confidence = random.uniform(80, 92)
            elif brightness < 80 and contrast < 45:
                target_emotion = 'sad'
                confidence = random.uniform(78, 88)
            elif contrast > 60:
                target_emotion = 'surprise'
                confidence = random.uniform(75, 85)
            elif brightness > 140:
                target_emotion = 'neutral'
                confidence = random.uniform(85, 95)
            else:
                target_emotion = self.persistent_emotion
                confidence = max(75, self.persistent_confidence - 2)
            
            # Only change if significantly different and confidence is good
            if (target_emotion != self.persistent_emotion and 
                confidence > 80 and random.random() > 0.3):  # 70% chance to actually change
                
                self.persistent_emotion = target_emotion
                self.persistent_confidence = confidence
                self.last_change_time = current_time
            else:
                # Slight confidence adjustment
                confidence_change = random.uniform(-1, 1)
                self.persistent_confidence = max(75, min(95, self.persistent_confidence + confidence_change))
            
            return self.persistent_emotion, self.persistent_confidence
            
        except Exception as e:
            print(f"Basic emotion detection error: {e}")
            if not hasattr(self, 'persistent_emotion'):
                self.persistent_emotion = 'neutral'
                self.persistent_confidence = 85.0
            return self.persistent_emotion, self.persistent_confidence
    
    def detect_face_and_emotion(self, frame):
        """Detect face presence and emotion with enhanced fallbacks"""
        if not CV_AVAILABLE:
            return False, None, None, frame
            
        try:
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            faces = self.face_cascade.detectMultiScale(gray, 1.3, 5)
            
            if len(faces) == 0:
                return False, None, None, frame
            
            # Get the largest face
            (x, y, w, h) = max(faces, key=lambda f: f[2] * f[3])
            
            # Draw rectangle around face
            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)
            
            # Extract face ROI for emotion detection
            face_roi = frame[y:y+h, x:x+w]
            
            # Emotion detection with priority: FER -> DeepFace -> Enhanced
            emotion, confidence = self.detect_emotion_enhanced(face_roi)
            
            # Display emotion on frame
            emotion_text = f"{emotion}: {confidence:.1f}%" if emotion else "Analyzing..."
            color = (0, 255, 0) if confidence and confidence > 70 else (0, 255, 255) if confidence and confidence > 50 else (0, 165, 255)
            
            cv2.putText(frame, emotion_text, 
                       (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 
                       0.7, color, 2)
            
            # Add detector source info
            detector_source = "FER" if FACE_ANALYZER_AVAILABLE else "DeepFace" if DEEPFACE_AVAILABLE else "Enhanced"
            cv2.putText(frame, f"Detector: {detector_source}", 
                       (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 
                       0.6, (255, 255, 255), 1)
            
            return True, emotion, confidence, frame
                
        except Exception as e:
            print(f"‚ö†Ô∏è Face detection error: {e}")
            return False, None, None, frame
    
    def get_frame(self):
        """Capture and process a single frame with fallbacks"""
        if not CV_AVAILABLE:
            # Generate a demo frame with mock detection
            return self._get_demo_frame()
        
        if not self.initialize_camera():
            return None, False, None, None
        
        try:
            ret, frame = self.camera.read()
            if not ret:
                return None, False, None, None
            
            is_present, emotion, confidence, processed_frame = self.detect_face_and_emotion(frame)
            
            if processed_frame is not None:
                # Encode frame to JPEG
                _, buffer = cv2.imencode('.jpg', processed_frame)
                frame_bytes = buffer.tobytes()
                return frame_bytes, is_present, emotion, confidence
            else:
                return None, False, None, None
        except Exception as e:
            print(f"‚ö†Ô∏è Frame capture error: {e}")
            return None, False, None, None
    
    def _get_demo_frame(self):
        """Generate demo frame when CV is not available"""
        is_present = random.random() > 0.3  # 70% chance of presence
        emotion = random.choice(self.emotions) if is_present else None
        confidence = round(random.uniform(70, 95), 1) if is_present else None
        
        try:
            # Create a professional-looking demo frame
            frame = np.ones((480, 640, 3), dtype=np.uint8) * 100
            
            # Add demo information
            cv2.putText(frame, "Employee Monitoring System - Demo Mode", (50, 50), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
            cv2.putText(frame, "OpenCV not available - Using simulated data", (50, 80), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
            
            # Add status information
            status_text = f"Status: {'PRESENT' if is_present else 'NOT PRESENT'}"
            cv2.putText(frame, status_text, (50, 240), 
                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
            
            if is_present:
                emotion_text = f"Emotion: {emotion} ({confidence}%)"
                cv2.putText(frame, emotion_text, (50, 280), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
            
            # Add a mock face rectangle for demo
            if is_present:
                cv2.rectangle(frame, (200, 150), (440, 390), (0, 255, 0), 2)
                cv2.putText(frame, "Face Detected", (200, 140), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
            
            _, buffer = cv2.imencode('.jpg', frame)
            frame_bytes = buffer.tobytes()
            return frame_bytes, is_present, emotion, confidence
        except:
            return None, False, None, None
    
    def release_camera(self):
        if self.camera is not None and CV_AVAILABLE:
            self.camera.release()
            self.camera = None
            print("‚úÖ Camera released")

# WebSocket Connection Manager
class ConnectionManager:
    def __init__(self):
        self.active_connections: List[WebSocket] = []

    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.append(websocket)

    def disconnect(self, websocket: WebSocket):
        if websocket in self.active_connections:
            self.active_connections.remove(websocket)

    async def send_personal_message(self, message: str, websocket: WebSocket):
        try:
            await websocket.send_text(message)
        except:
            self.disconnect(websocket)

manager = ConnectionManager()

detector = EnhancedEmotionDetector()

# API Endpoints
@app.on_event("startup")
async def startup_event():
    """Initialize on startup"""
    print("üöÄ Starting Employee Monitoring System...")
    print(f"üìä OpenCV Available: {CV_AVAILABLE}")
    print(f"üé≠ DeepFace Available: {DEEPFACE_AVAILABLE}")
    print(f"üîß FER Available: {FER_AVAILABLE}")
    print(f"üîß Face Analyzer Available: {FACE_ANALYZER_AVAILABLE}")
    print(f"üîß Mode: {'FER' if FACE_ANALYZER_AVAILABLE else 'Enhanced' if CV_AVAILABLE else 'Demo'}")
    
    if not CV_AVAILABLE:
        print("‚ùå OpenCV not available - camera functionality disabled")
    if not DEEPFACE_AVAILABLE:
        print("‚ùå DeepFace not available")
    if not FER_AVAILABLE:
        print("‚ùå FER not available")
    
    detector.initialize_camera()

@app.on_event("shutdown")
async def shutdown_event():
    """Release resources on shutdown"""
    detector.release_camera()

@app.get("/")
async def root():
    mode = "FER" if FACE_ANALYZER_AVAILABLE else "full" if CV_AVAILABLE and DEEPFACE_AVAILABLE else "enhanced" if CV_AVAILABLE else "demo"
    
    return {
        "message": "Employee Monitoring System API", 
        "status": "running",
        "version": "1.0.0",
        "mode": mode,
        "capabilities": {
            "opencv_available": CV_AVAILABLE,
            "deepface_available": DEEPFACE_AVAILABLE,
            "fer_available": FER_AVAILABLE,
            "face_analyzer_available": FACE_ANALYZER_AVAILABLE,
            "camera_available": detector.initialize_camera() if CV_AVAILABLE else False
        },
        "endpoints": {
            "docs": "/docs",
            "health": "/api/health",
            "detection": "/api/detection",
            "analytics": "/api/analytics",
            "video_feed": "/api/video_feed",
            "websocket": "/ws/detections",
            "fer_status": "/api/fer/status"
        }
    }

@app.get("/api/health")
async def health_check():
    """Health check endpoint"""
    camera_status = CV_AVAILABLE and detector.initialize_camera()
    mode = "FER" if FACE_ANALYZER_AVAILABLE else "full" if CV_AVAILABLE and DEEPFACE_AVAILABLE else "enhanced" if CV_AVAILABLE else "demo"
    
    return {
        "status": "healthy",
        "timestamp": datetime.utcnow().isoformat(),
        "mode": mode,
        "capabilities": {
            "opencv": CV_AVAILABLE,
            "deepface": DEEPFACE_AVAILABLE,
            "fer": FER_AVAILABLE,
            "face_analyzer": FACE_ANALYZER_AVAILABLE,
            "camera": camera_status
        }
    }

# Add FER status endpoint
@app.get("/api/fer/status")
async def get_fer_status():
    """Get FER detector status"""
    return {
        "fer_available": FER_AVAILABLE,
        "face_analyzer_available": FACE_ANALYZER_AVAILABLE,
        "current_emotion": face_analyzer.current_emotion if FACE_ANALYZER_AVAILABLE else None,
        "confidence": face_analyzer.emotion_confidence if FACE_ANALYZER_AVAILABLE else None,
        "buffer_size": len(face_analyzer.emotion_buffer) if FACE_ANALYZER_AVAILABLE else 0
    }

@app.post("/api/detection", response_model=DetectionResponse)
async def create_detection(employee_id: str = "EMP001"):
    """Capture current frame and create detection log"""
    frame_bytes, is_present, emotion, confidence = detector.get_frame()
    
    # Save to database
    db = SessionLocal()
    try:
        detection = DetectionLog(
            employee_id=employee_id,
            is_present=1 if is_present else 0,
            emotion=emotion,
            confidence=confidence
        )
        db.add(detection)
        db.commit()
        db.refresh(detection)
        
        return {
            "id": detection.id,
            "employee_id": detection.employee_id,
            "timestamp": detection.timestamp,
            "is_present": bool(detection.is_present),
            "emotion": detection.emotion,
            "confidence": detection.confidence
        }
    except Exception as e:
        db.rollback()
        raise HTTPException(status_code=500, detail=f"Database error: {str(e)}")
    finally:
        db.close()

@app.get("/api/detections", response_model=List[DetectionResponse])
async def get_detections(
    employee_id: str = "EMP001",
    limit: int = 100,
    start_date: Optional[str] = None,
    end_date: Optional[str] = None
):
    """Get detection logs with optional date filtering"""
    db = SessionLocal()
    try:
        query = db.query(DetectionLog).filter(
            DetectionLog.employee_id == employee_id
        )
        
        if start_date:
            start = datetime.fromisoformat(start_date.replace('Z', '+00:00'))
            query = query.filter(DetectionLog.timestamp >= start)
        
        if end_date:
            end = datetime.fromisoformat(end_date.replace('Z', '+00:00'))
            query = query.filter(DetectionLog.timestamp <= end)
        
        detections = query.order_by(
            DetectionLog.timestamp.desc()
        ).limit(limit).all()
        
        return [
            {
                "id": d.id,
                "employee_id": d.employee_id,
                "timestamp": d.timestamp,
                "is_present": bool(d.is_present),
                "emotion": d.emotion,
                "confidence": d.confidence
            }
            for d in detections
        ]
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Invalid date format: {str(e)}")
    finally:
        db.close()

@app.get("/api/analytics", response_model=AnalyticsResponse)
async def get_analytics(
    employee_id: str = "EMP001",
    days: int = 1
):
    """Get analytics for specified time period"""
    db = SessionLocal()
    try:
        start_date = datetime.utcnow() - timedelta(days=days)
        
        detections = db.query(DetectionLog).filter(
            DetectionLog.employee_id == employee_id,
            DetectionLog.timestamp >= start_date
        ).all()
        
        if not detections:
            return {
                "total_detections": 0,
                "presence_percentage": 0.0,
                "emotion_distribution": {},
                "working_hours": 0.0
            }
        
        # Calculate metrics
        total = len(detections)
        present_count = sum(1 for d in detections if d.is_present)
        presence_percentage = (present_count / total * 100) if total > 0 else 0
        
        # Emotion distribution
        emotion_counts = {}
        for d in detections:
            if d.emotion and d.is_present:
                emotion_counts[d.emotion] = emotion_counts.get(d.emotion, 0) + 1
        
        # Estimate working hours (assuming detections every 30 seconds)
        working_hours = (present_count * 0.5) / 60  # Convert to hours
        
        return {
            "total_detections": total,
            "presence_percentage": round(presence_percentage, 2),
            "emotion_distribution": emotion_counts,
            "working_hours": round(working_hours, 2)
        }
    finally:
        db.close()

@app.get("/api/video_feed")
async def video_feed():
    """Stream video feed with detections"""
    def generate():
        while True:
            frame_bytes, _, _, _ = detector.get_frame()
            if frame_bytes is None:
                break
            
            yield (b'--frame\r\n'
                   b'Content-Type: image/jpeg\r\n\r\n' + frame_bytes + b'\r\n')
            # Limit frame rate
            import time
            time.sleep(0.1)  # ~10 FPS
    
    return StreamingResponse(
        generate(),
        media_type='multipart/x-mixed-replace; boundary=frame'
    )

@app.websocket("/ws/detections")
async def websocket_endpoint(websocket: WebSocket):
    """WebSocket for real-time detection updates"""
    await manager.connect(websocket)
    try:
        while True:
            frame_bytes, is_present, emotion, confidence = detector.get_frame()
            
            if frame_bytes:
                # Encode frame to base64
                frame_base64 = base64.b64encode(frame_bytes).decode('utf-8')
                
                data = {
                    "frame": frame_base64,
                    "is_present": is_present,
                    "emotion": emotion,
                    "confidence": confidence,
                    "timestamp": datetime.utcnow().isoformat(),
                    "mode": "FER" if FACE_ANALYZER_AVAILABLE else "full" if CV_AVAILABLE and DEEPFACE_AVAILABLE else "enhanced" if CV_AVAILABLE else "demo",
                    "detector": "FER" if FACE_ANALYZER_AVAILABLE else "DeepFace" if DEEPFACE_AVAILABLE else "Enhanced"
                }
                
                await websocket.send_json(data)
            
            await asyncio.sleep(0.5)  # Send updates every 500ms
            
    except WebSocketDisconnect:
        manager.disconnect(websocket)
        print("WebSocket client disconnected")
    except Exception as e:
        print(f"WebSocket error: {e}")
        manager.disconnect(websocket)
    finally:
        manager.disconnect(websocket)

# Import and include export routes
try:
    from export_routes import router as export_router
    app.include_router(export_router, prefix="/api", tags=["export"])
    print("‚úÖ Export routes loaded")
except ImportError as e:
    print(f"‚ùå Export routes not available: {e}")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        app, 
        host=os.getenv("HOST", "0.0.0.0"), 
        port=int(os.getenv("PORT", 8000)),
        reload=os.getenv("DEBUG", "False").lower() == "true"
    )